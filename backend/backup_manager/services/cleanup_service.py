"""
Service de nettoyage automatique des fichiers temporaires
"""

import os
import shutil
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Union, Set, Iterator
from django.conf import settings
from .base_service import BaseService


class CleanupService(BaseService):
    """Service pour nettoyer automatiquement les fichiers temporaires"""
    
    # Configuration des r√©pertoires et leurs param√®tres de nettoyage
    CLEANUP_CONFIGS = {
        'restore_temp': {'max_age_hours': 24, 'recursive': True},
        'temp': {'max_age_hours': 24, 'recursive': True},
        'uploads': {'max_age_hours': 2, 'recursive': True},
        'storage': {'max_age_hours': 1, 'recursive': False, 'orphan_check': True}
    }
    
    # Cache TTL en secondes (5 minutes)
    CACHE_TTL = 300
    
    def __init__(self):
        super().__init__('CleanupService')
        self.backup_root = self._validate_backup_root()
        # Cache optimis√© pour les fichiers r√©f√©renc√©s
        self._referenced_files_cache: Optional[Set[Path]] = None
        self._cache_timestamp: Optional[datetime] = None
    
    def _validate_backup_root(self) -> Path:
        """Valide et retourne le r√©pertoire racine des backups"""
        if not hasattr(settings, 'BACKUP_ROOT'):
            raise ValueError("BACKUP_ROOT n'est pas configur√© dans les settings Django")
        
        backup_root = Path(settings.BACKUP_ROOT)
        backup_root.mkdir(parents=True, exist_ok=True)
        return backup_root
    
    def _calculate_cutoff_time(self, max_age_hours: int) -> datetime:
        """Calcule le temps de coupure pour un √¢ge maximum donn√©"""
        return datetime.now() - timedelta(hours=max_age_hours)
    
    def _get_file_stats_safe(self, file_path: Path) -> Tuple[datetime, int, bool]:
        """R√©cup√®re l'√¢ge, la taille et l'existence d'un fichier de mani√®re s√©curis√©e"""
        try:
            stat_info = file_path.stat()
            return (
                datetime.fromtimestamp(stat_info.st_mtime),
                stat_info.st_size,
                True
            )
        except OSError:
            return datetime.now(), 0, False
    
    def _is_file_old_enough(self, file_path: Path, cutoff_time: datetime) -> bool:
        """V√©rifie si un fichier est assez ancien pour √™tre supprim√©"""
        file_time, _, exists = self._get_file_stats_safe(file_path)
        return exists and file_time < cutoff_time
    
    def _get_directory_stats_optimized(self, directory: Path, max_depth: Optional[int] = None) -> Tuple[int, int]:
        """
        Calcule la taille ET le nombre de fichiers d'un r√©pertoire de mani√®re optimis√©e
        
        Args:
            directory: R√©pertoire √† analyser
            max_depth: Profondeur maximale (None = illimit√©e)
        
        Returns:
            Tuple (taille_totale, nombre_fichiers)
        """
        if not directory.exists():
            return 0, 0
        
        total_size = 0
        file_count = 0
        
        try:
            # Utiliser un g√©n√©rateur avec limitation de profondeur si sp√©cifi√©e
            if max_depth is not None:
                file_iterator = self._limited_rglob(directory, max_depth)
            else:
                file_iterator = directory.rglob("*")
            
            for file_path in file_iterator:
                if file_path.is_file():
                    try:
                        total_size += file_path.stat().st_size
                        file_count += 1
                    except OSError:
                        # Compter le fichier m√™me si on ne peut pas avoir sa taille
                        file_count += 1
        except OSError as e:
            self.log_warning(f"‚ö†Ô∏è Erreur lors du parcours de {directory}: {e}")
        
        return total_size, file_count
    
    def _limited_rglob(self, directory: Path, max_depth: int) -> Iterator[Path]:
        """G√©n√©rateur pour parcourir un r√©pertoire avec une profondeur limit√©e"""
        def _walk_limited(path: Path, current_depth: int):
            if current_depth > max_depth:
                return
            
            try:
                for item in path.iterdir():
                    yield item
                    if item.is_dir():
                        yield from _walk_limited(item, current_depth + 1)
            except OSError:
                pass
        
        return _walk_limited(directory, 0)
    
    def _get_referenced_files_cached(self) -> Set[Path]:
        """R√©cup√®re tous les fichiers r√©f√©renc√©s en base avec cache optimis√©"""
        now = datetime.now()
        
        # V√©rifier la validit√© du cache
        if (self._referenced_files_cache is not None and 
            self._cache_timestamp is not None and 
            (now - self._cache_timestamp).total_seconds() < self.CACHE_TTL):
            return self._referenced_files_cache
        
        # Recharger le cache
        self.log_info("üîÑ Rechargement du cache des fichiers r√©f√©renc√©s")
        
        try:
            from ..models import BackupHistory
            
            referenced_files = set()
            # Optimiser la requ√™te avec select_related si n√©cessaire
            for backup in BackupHistory.objects.filter(file_path__isnull=False).only('file_path'):
                if backup.file_path:
                    # Normaliser le chemin
                    if os.path.isabs(backup.file_path):
                        referenced_files.add(Path(backup.file_path))
                    else:
                        referenced_files.add(self.backup_root / backup.file_path)
            
            # Mettre √† jour le cache
            self._referenced_files_cache = referenced_files
            self._cache_timestamp = now
            
            self.log_info(f"‚úÖ Cache recharg√©: {len(referenced_files)} fichiers r√©f√©renc√©s")
            return referenced_files
            
        except Exception as e:
            self.log_error(f"‚ùå Erreur lors du rechargement du cache: {e}")
            return self._referenced_files_cache or set()
    
    def _should_delete_file(self, file_path: Path, cutoff_time: datetime, 
                           context: str = "general") -> bool:
        """
        D√©termine si un fichier doit √™tre supprim√© selon le contexte
        
        Args:
            file_path: Chemin du fichier
            cutoff_time: Temps de coupure
            context: Contexte ('orphan', 'decrypted', 'general')
        """
        if not file_path.is_file():
            return False
        
        # V√©rifications communes
        if not self._is_file_old_enough(file_path, cutoff_time):
            return False
        
        # V√©rifications sp√©cifiques au contexte
        if context == "orphan":
            referenced_files = self._get_referenced_files_cached()
            return file_path not in referenced_files
        
        elif context == "decrypted":
            filename_lower = file_path.name.lower()
            return 'decrypted' in filename_lower or 'temp' in filename_lower
        
        return True  # Contexte g√©n√©ral
    
    def _cleanup_directory_generic(self, directory: Path, cutoff_time: datetime,
                                  context: str = "general") -> Dict[str, int]:
        """
        M√©thode g√©n√©rique pour nettoyer un r√©pertoire
        
        Args:
            directory: R√©pertoire √† nettoyer
            cutoff_time: Temps de coupure
            context: Contexte pour les r√®gles de suppression
        """
        if not directory.exists():
            return {'files_deleted': 0, 'size_freed': 0, 'directories_removed': 0}
        
        stats = {'files_deleted': 0, 'size_freed': 0, 'directories_removed': 0}
        
        try:
            for item in directory.iterdir():
                if not self._is_file_old_enough(item, cutoff_time):
                    continue
                
                if item.is_file():
                    if self._should_delete_file(item, cutoff_time, context):
                        self._delete_file_safe(item, stats)
                elif item.is_dir():
                    self._delete_directory_safe(item, stats)
                    
        except OSError as e:
            self.log_warning(f"‚ö†Ô∏è Erreur lors du parcours de {directory}: {e}")
        
        return stats
    
    def _delete_file_safe(self, file_path: Path, stats: Dict[str, int]):
        """Supprime un fichier de mani√®re s√©curis√©e et met √† jour les stats"""
        try:
            _, file_size, exists = self._get_file_stats_safe(file_path)
            if not exists:
                return
            
            self.log_info(f"  üóëÔ∏è Suppression: {file_path.name} ({self.format_size(file_size)})")
            file_path.unlink()
            
            stats['files_deleted'] += 1
            stats['size_freed'] += file_size
            
        except OSError as e:
            self.log_warning(f"‚ö†Ô∏è Impossible de supprimer {file_path}: {e}")
    
    def _delete_directory_safe(self, dir_path: Path, stats: Dict[str, int]):
        """Supprime un r√©pertoire de mani√®re s√©curis√©e et met √† jour les stats"""
        try:
            # Calculer les stats avant suppression
            dir_size, file_count = self._get_directory_stats_optimized(dir_path)
            
            self.log_info(f"  üóëÔ∏è Suppression: {dir_path.name}/ ({self.format_size(dir_size)}, {file_count} fichiers)")
            shutil.rmtree(dir_path)
            
            stats['files_deleted'] += file_count
            stats['size_freed'] += dir_size
            stats['directories_removed'] += 1
            
        except OSError as e:
            self.log_warning(f"‚ö†Ô∏è Impossible de supprimer {dir_path}: {e}")
    
    def cleanup_all_temporary_files(self, max_age_hours: int = 24) -> Dict[str, Union[Dict[str, int], int, float]]:
        """
        Nettoie tous les fichiers temporaires de mani√®re optimis√©e
        
        Args:
            max_age_hours: √Çge maximum en heures avant suppression
            
        Returns:
            Dictionnaire avec les statistiques de nettoyage
        """
        self.start_operation("Nettoyage automatique des fichiers temporaires")
        
        # Ex√©cuter les nettoyages en parall√®le logique
        results = {}
        
        # Nettoyage des r√©pertoires standards
        for dir_name, config in self.CLEANUP_CONFIGS.items():
            if dir_name == 'storage':
                continue  # Trait√© s√©par√©ment pour les orphelins
            
            age_hours = config.get('max_age_hours', max_age_hours)
            results[dir_name] = self._cleanup_standard_directory(dir_name, age_hours)
        
        # Nettoyage sp√©cialis√©
        results['orphaned_files'] = self.cleanup_orphaned_files()
        results['decrypted_files'] = self.cleanup_decrypted_files_optimized(max_age_hours=1)
        
        # Calculer les totaux
        total_files = sum(r.get('files_deleted', 0) for r in results.values())
        total_size = sum(r.get('size_freed', 0) for r in results.values())
        
        duration = self.end_operation("Nettoyage automatique des fichiers temporaires")
        
        self.log_info(f"‚úÖ Nettoyage termin√© en {duration}s: {total_files} fichiers supprim√©s, {self.format_size(total_size)} lib√©r√©s")
        
        return {
            **results,
            'totals': {
                'files_deleted': total_files,
                'size_freed': total_size,
                'duration_seconds': duration
            }
        }
    
    def _cleanup_standard_directory(self, dir_name: str, max_age_hours: int) -> Dict[str, int]:
        """Nettoie un r√©pertoire standard selon sa configuration"""
        directory = self.backup_root / dir_name
        cutoff_time = self._calculate_cutoff_time(max_age_hours)
        
        self.log_info(f"üìÅ Nettoyage {dir_name} (fichiers > {max_age_hours}h)")
        stats = self._cleanup_directory_generic(directory, cutoff_time)
        
        self.log_info(f"‚úÖ {dir_name}: {stats['files_deleted']} fichiers supprim√©s, "
                     f"{self.format_size(stats['size_freed'])} lib√©r√©s")
        return stats
    
    def cleanup_decrypted_files_optimized(self, max_age_hours: int = 1) -> Dict[str, int]:
        """
        Version optimis√©e du nettoyage des fichiers d√©chiffr√©s
        √âvite le rglob sur tout l'arbre de fichiers
        """
        cutoff_time = self._calculate_cutoff_time(max_age_hours)
        self.log_info(f"üîì Nettoyage fichiers d√©chiffr√©s temporaires (> {max_age_hours}h)")
        
        stats = {'files_deleted': 0, 'size_freed': 0}
        
        # Chercher seulement dans les r√©pertoires susceptibles de contenir ces fichiers
        search_dirs = ['temp', 'restore_temp', 'uploads']
        
        for dir_name in search_dirs:
            search_dir = self.backup_root / dir_name
            if not search_dir.exists():
                continue
                
            try:
                # Limiter la profondeur pour √©viter les parcours trop longs
                for file_path in self._limited_rglob(search_dir, max_depth=3):
                    if self._should_delete_file(file_path, cutoff_time, "decrypted"):
                        self._delete_file_safe(file_path, stats)
                        
            except OSError as e:
                self.log_warning(f"‚ö†Ô∏è Erreur lors du parcours de {search_dir}: {e}")
        
        self.log_info(f"‚úÖ d√©chiffr√©s: {stats['files_deleted']} fichiers supprim√©s, {self.format_size(stats['size_freed'])} lib√©r√©s")
        return stats
    
    def cleanup_orphaned_files(self) -> Dict[str, int]:
        """Supprime les fichiers orphelins (non r√©f√©renc√©s en base) de mani√®re optimis√©e"""
        cutoff_time = self._calculate_cutoff_time(1)
        self.log_info("üîç Recherche de fichiers orphelins")
        
        storage_dir = self.backup_root / "storage"
        if not storage_dir.exists():
            return {'files_deleted': 0, 'size_freed': 0}
        
        stats = {'files_deleted': 0, 'size_freed': 0}
        
        try:
            # Utiliser un parcours limit√© pour de meilleures performances
            for file_path in self._limited_rglob(storage_dir, max_depth=5):
                if self._should_delete_file(file_path, cutoff_time, "orphan"):
                    self._delete_file_safe(file_path, stats)
                    
        except OSError as e:
            self.log_warning(f"‚ö†Ô∏è Erreur lors du parcours du storage: {e}")
        
        self.log_info(f"‚úÖ orphelins: {stats['files_deleted']} fichiers supprim√©s, {self.format_size(stats['size_freed'])} lib√©r√©s")
        return stats
    
    def dry_run_cleanup(self, max_age_hours: int = 24) -> Dict[str, Union[Dict[str, int], str]]:
        """
        Simule le nettoyage sans supprimer les fichiers de mani√®re optimis√©e
        """
        self.log_info("üß™ Simulation de nettoyage (dry run)")
        
        results = {}
        
        # Simulation pour les r√©pertoires standards
        for dir_name, config in self.CLEANUP_CONFIGS.items():
            if dir_name == 'storage':
                continue
            
            age_hours = config.get('max_age_hours', max_age_hours)
            cutoff_time = self._calculate_cutoff_time(age_hours)
            results[dir_name] = self._dry_run_directory_optimized(
                self.backup_root / dir_name, cutoff_time
            )
        
        # Simulations sp√©cialis√©es
        results['orphaned_files'] = self._dry_run_orphaned_files_optimized()
        results['decrypted_files'] = self._dry_run_decrypted_files_optimized(max_age_hours=1)
        
        # Calculer les totaux
        total_files = sum(r.get('files_to_delete', 0) for r in results.values())
        total_size = sum(r.get('size_to_free', 0) for r in results.values())
        
        return {
            **results,
            'totals': {
                'files_to_delete': total_files,
                'size_to_free': total_size,
                'size_to_free_formatted': self.format_size(total_size)
            }
        }
    
    def _dry_run_directory_optimized(self, directory: Path, cutoff_time: datetime) -> Dict[str, int]:
        """Version optimis√©e de la simulation de nettoyage d'un r√©pertoire"""
        if not directory.exists():
            return {'files_to_delete': 0, 'size_to_free': 0, 'directories_to_remove': 0}
        
        files_to_delete = 0
        size_to_free = 0
        directories_to_remove = 0
        
        try:
            for item in directory.iterdir():
                if not self._is_file_old_enough(item, cutoff_time):
                    continue
                
                if item.is_file():
                    _, file_size, exists = self._get_file_stats_safe(item)
                    if exists:
                        files_to_delete += 1
                        size_to_free += file_size
                elif item.is_dir():
                    dir_size, file_count = self._get_directory_stats_optimized(item)
                    files_to_delete += file_count
                    size_to_free += dir_size
                    directories_to_remove += 1
                    
        except OSError as e:
            self.log_warning(f"‚ö†Ô∏è Erreur lors de la simulation pour {directory}: {e}")
        
        return {
            'files_to_delete': files_to_delete,
            'size_to_free': size_to_free,
            'directories_to_remove': directories_to_remove
        }
    
    def _dry_run_orphaned_files_optimized(self) -> Dict[str, int]:
        """Version optimis√©e de la simulation pour les fichiers orphelins"""
        cutoff_time = self._calculate_cutoff_time(1)
        storage_dir = self.backup_root / "storage"
        
        if not storage_dir.exists():
            return {'files_to_delete': 0, 'size_to_free': 0}
        
        files_to_delete = 0
        size_to_free = 0
        
        try:
            for file_path in self._limited_rglob(storage_dir, max_depth=5):
                if self._should_delete_file(file_path, cutoff_time, "orphan"):
                    files_to_delete += 1
                    _, file_size, _ = self._get_file_stats_safe(file_path)
                    size_to_free += file_size
                    
        except OSError as e:
            self.log_warning(f"‚ö†Ô∏è Erreur lors de la simulation des orphelins: {e}")
        
        return {'files_to_delete': files_to_delete, 'size_to_free': size_to_free}
    
    def _dry_run_decrypted_files_optimized(self, max_age_hours: int = 1) -> Dict[str, int]:
        """Version optimis√©e de la simulation pour les fichiers d√©chiffr√©s"""
        cutoff_time = self._calculate_cutoff_time(max_age_hours)
        files_to_delete = 0
        size_to_free = 0
        
        search_dirs = ['temp', 'restore_temp', 'uploads']
        
        for dir_name in search_dirs:
            search_dir = self.backup_root / dir_name
            if not search_dir.exists():
                continue
                
            try:
                for file_path in self._limited_rglob(search_dir, max_depth=3):
                    if self._should_delete_file(file_path, cutoff_time, "decrypted"):
                        files_to_delete += 1
                        _, file_size, _ = self._get_file_stats_safe(file_path)
                        size_to_free += file_size
                        
            except OSError as e:
                self.log_warning(f"‚ö†Ô∏è Erreur simulation d√©chiffr√©s dans {search_dir}: {e}")
        
        return {'files_to_delete': files_to_delete, 'size_to_free': size_to_free}
    
    def cleanup_restore_temp(self, max_age_hours: int = 24) -> Dict[str, int]:
        """Nettoie les fichiers temporaires de restauration (wrapper de compatibilit√©)"""
        return self._cleanup_standard_directory('restore_temp', max_age_hours)
    
    def cleanup_temp_files(self, max_age_hours: int = 24) -> Dict[str, int]:
        """Nettoie les fichiers temporaires g√©n√©raux (wrapper de compatibilit√©)"""
        return self._cleanup_standard_directory('temp', max_age_hours)
    
    def cleanup_upload_files(self, max_age_hours: int = 2) -> Dict[str, int]:
        """Nettoie les fichiers d'upload (wrapper de compatibilit√©)"""
        return self._cleanup_standard_directory('uploads', max_age_hours)
    
    def cleanup_decrypted_files(self, max_age_hours: int = 1) -> Dict[str, int]:
        """Nettoie les fichiers d√©chiffr√©s temporaires (wrapper de compatibilit√©)"""
        return self.cleanup_decrypted_files_optimized(max_age_hours)
    
    def get_directory_size(self, directory: Path) -> int:
        """Calcule la taille d'un r√©pertoire de mani√®re optimis√©e"""
        size, _ = self._get_directory_stats_optimized(directory)
        return size
    
    def count_files_in_directory(self, directory: Path) -> int:
        """Compte les fichiers dans un r√©pertoire de mani√®re optimis√©e"""
        _, count = self._get_directory_stats_optimized(directory)
        return count
    
    def format_size(self, size_bytes: int) -> str:
        """Formate une taille en bytes en format lisible"""
        if size_bytes == 0:
            return "0 B"
        
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        
        return f"{size_bytes:.1f} PB"
    
    def get_cleanup_stats(self) -> Dict[str, Dict[str, Union[int, str]]]:
        """Obtient les statistiques de l'espace disque et des fichiers temporaires"""
        stats = {}
        
        for subdir in ['restore_temp', 'temp', 'uploads', 'storage']:
            dir_path = self.backup_root / subdir
            if dir_path.exists():
                # Utiliser la m√©thode optimis√©e
                size, file_count = self._get_directory_stats_optimized(dir_path)
                stats[subdir] = {
                    'size': size,
                    'size_formatted': self.format_size(size),
                    'file_count': file_count
                }
            else:
                stats[subdir] = {
                    'size': 0,
                    'size_formatted': '0 B',
                    'file_count': 0
                }
        
        return stats
    
    def clear_cache(self):
        """Efface le cache des fichiers r√©f√©renc√©s"""
        self._referenced_files_cache = None
        self._cache_timestamp = None
        self.log_info("üîÑ Cache des fichiers r√©f√©renc√©s effac√©") 